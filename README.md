---
title: Parakeet-tdt-0.6b-v3 ONNX CPU
emoji: ü¶Ä
colorFrom: green
colorTo: gray
sdk: gradio
sdk_version: 5.49.0
app_file: app.py
pinned: false
license: bsd-3-clause
short_description: Speech transcription (Nvidia/parakeet-tdt-0.6b-v3+onnx_asr)
tags:
- asr
- onnx
preload_from_hub:
- istupakov/parakeet-tdt-0.6b-v3-onnx
models:
- istupakov/parakeet-tdt-0.6b-v3-onnx
---

This space uses Nvidia [parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3) model in [onnx format](https://huggingface.co/istupakov/parakeet-tdt-0.6b-v3-onnx) with [onnx-asr](https://github.com/istupakov/onnx-asr) backend.
In theory with minor edits it can work both on CPU and GPU but I don't have access to ZeroGPU spaces to enable hardware acceleration.
Locally I tested it with Nvidia RTX3060 and RTX4060Ti and it used about 6GB VRAM with 150s chunks.

---

# Parakeet ‚Äî Transcription & Analyse temps r√©el

Application de bureau (Windows / macOS) pour la transcription en direct de r√©unions avec d√©tection automatique des questions et actions, propuls√©e par le mod√®le ASR **NeMo Parakeet TDT 0.6B v3** (ONNX) et un LLM local via **LMStudio**.

---

## Table des mati√®res

1. [Pr√©requis syst√®me](#1-pr√©requis-syst√®me)
2. [Installation de Node.js](#2-installation-de-nodejs)
3. [Installation de Python](#3-installation-de-python)
4. [Cloner / d√©compresser le projet](#4-cloner--d√©compresser-le-projet)
5. [Installer les d√©pendances Python](#5-installer-les-d√©pendances-python)
6. [Installer les d√©pendances Node.js](#6-installer-les-d√©pendances-nodejs)
7. [Installer et configurer LMStudio](#7-installer-et-configurer-lmstudio)
8. [Premier lancement](#8-premier-lancement)
9. [Lancement macOS (Apple M3)](#9-lancement-macos-apple-m3)
10. [Sources audio support√©es](#10-sources-audio-support√©es)
11. [FAQ / Probl√®mes courants](#11-faq--probl√®mes-courants)

---

## 1. Pr√©requis syst√®me

| Composant | Windows | macOS (M3) |
|-----------|---------|------------|
| OS | Windows 10/11 64-bit | macOS 14 Sonoma ou plus r√©cent |
| GPU (optionnel) | NVIDIA CUDA 11.8+ | ‚Äî (CPU uniquement, Metal non support√© par ONNX Runtime) |
| RAM | 8 Go minimum, 16 Go recommand√© | 8 Go minimum, 16 Go recommand√© |
| Espace disque | ~5 Go (mod√®le ~600 Mo + LMStudio) | ~5 Go |
| Python | 3.10 ‚Äì 3.12 | 3.10 ‚Äì 3.12 |
| Node.js | 18 LTS ou 20 LTS | 18 LTS ou 20 LTS |

> **GPU NVIDIA** : si vous disposez d'une carte NVIDIA, `onnx-asr` utilisera CUDA automatiquement pour acc√©l√©rer la transcription. Sans GPU la transcription tourne sur CPU (plus lent mais fonctionnel).

---

## 2. Installation de Node.js

### Windows
T√©l√©chargez l'installateur LTS depuis [https://nodejs.org](https://nodejs.org) et ex√©cutez-le.
V√©rification :
```cmd
node -v   # ex. v20.11.0
npm -v    # ex. 10.2.4
```

### macOS
```bash
# Via Homebrew (recommand√©)
brew install node@20
# Ou via nvm (gestionnaire de versions)
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash
source ~/.zshrc
nvm install 20
nvm use 20
```

---

## 3. Installation de Python

### Windows
T√©l√©chargez Python 3.11 depuis [https://www.python.org/downloads/](https://www.python.org/downloads/).
**Important** : cochez "Add Python to PATH" lors de l'installation.
V√©rification :
```cmd
python --version   # Python 3.11.x
```

### macOS (M3)
```bash
# Via Homebrew
brew install python@3.11
# V√©rification
python3 --version  # Python 3.11.x
```

---

## 4. Cloner / d√©compresser le projet

```bash
# Via Git
git clone https://github.com/votre-repo/parakeet-tdt-0.6b-v3-onnx-cpu.git
cd parakeet-tdt-0.6b-v3-onnx-cpu

# Ou d√©compressez l'archive ZIP dans le dossier de votre choix
```

---

## 5. Installer les d√©pendances Python

Le projet utilise un **environnement virtuel** (`venv`) isol√© du Python syst√®me.

### Windows
```cmd
cd parakeet-tdt-0.6b-v3-onnx-cpu

:: Cr√©er l'environnement virtuel
python -m venv venv

:: Activer l'environnement
venv\Scripts\activate

:: Installer les d√©pendances
pip install -r requirements.txt
```

> **Avec GPU NVIDIA** : `onnx-asr[gpu,hub]` installe automatiquement `onnxruntime-gpu`. Assurez-vous que CUDA 11.8+ est install√©.
> **Sans GPU** : remplacez `onnx-asr[gpu,hub]` par `onnx-asr[hub]` dans `requirements.txt` avant l'installation.

### macOS (M3)
```bash
cd parakeet-tdt-0.6b-v3-onnx-cpu

# Cr√©er l'environnement virtuel
python3 -m venv venv

# Activer l'environnement
source venv/bin/activate

# Installer les d√©pendances (CPU uniquement sur M3)
pip install "onnx-asr[hub]>=0.6.1" "fastapi>=0.115.0" "uvicorn[standard]>=0.30.0"
```

> Sur Apple Silicon, `onnxruntime` s'ex√©cute en mode CPU. La directive `[gpu]` n'est pas n√©cessaire.

Le premier lancement t√©l√©chargera automatiquement le mod√®le **nemo-parakeet-tdt-0.6b-v3** (~600 Mo) depuis HuggingFace Hub.

---

## 6. Installer les d√©pendances Node.js

```bash
cd electron
npm install
```

Cela installe uniquement **Electron ^32** (d√©fini dans `electron/package.json`). Aucune d√©pendance native n'est requise.

---

## 7. Installer et configurer LMStudio

LMStudio est le serveur LLM local qui fournit les r√©ponses aux questions d√©tect√©es et g√©n√®re le r√©sum√© de r√©union.

### 7.1 Installation automatique (scripts fournis)

#### Windows
```cmd
install-lmstudio.bat
```

#### macOS
```bash
chmod +x install-lmstudio.sh
./install-lmstudio.sh
```

Ces scripts t√©l√©chargent et installent LMStudio automatiquement.

### 7.2 Installation manuelle

1. Rendez-vous sur [https://lmstudio.ai](https://lmstudio.ai)
2. T√©l√©chargez la version correspondant √† votre syst√®me :
   - **Windows** : `LM-Studio-x.x.x-Setup.exe`
   - **macOS (Apple Silicon)** : `LM-Studio-x.x.x-arm64.dmg`
3. Installez l'application

### 7.3 Chargement d'un mod√®le LLM

Parakeet fonctionne avec n'importe quel mod√®le compatible **OpenAI chat completions** charg√© dans LMStudio.

**Mod√®les recommand√©s** (bon √©quilibre vitesse / qualit√©) :

| Mod√®le | Taille | RAM n√©cessaire |
|--------|--------|---------------|
| `Mistral 7B Instruct v0.3` (Q4_K_M) | ~4.4 Go | 8 Go |
| `Llama 3.1 8B Instruct` (Q4_K_M) | ~4.9 Go | 8 Go |
| `Qwen2.5 7B Instruct` (Q4_K_M) | ~4.5 Go | 8 Go |
| `Phi-3.5 Mini Instruct` (Q4_K_M) | ~2.2 Go | 6 Go |

**√âtapes dans LMStudio :**

1. Ouvrez LMStudio
2. Onglet **Search** (loupe) ‚Üí cherchez le mod√®le souhait√© (ex. `mistral-7b-instruct`)
3. Cliquez **Download** sur la variante `Q4_K_M`
4. Une fois t√©l√©charg√©, onglet **Local Server** (ic√¥ne `<->`)
5. S√©lectionnez le mod√®le dans le menu d√©roulant
6. Cliquez **Start Server** ‚Üí le serveur d√©marre sur `http://localhost:1234`

> L'URL `http://localhost:1234/v1` est pr√©-configur√©e dans Parakeet. Vous pouvez la modifier dans l'interface si n√©cessaire.

### 7.4 D√©marrage automatique du serveur LMStudio

Pour que LMStudio d√©marre son serveur au lancement :

- Dans LMStudio : `Settings > Local Server > Start server on app startup`

---

## 8. Premier lancement

### Windows
Double-cliquez sur `start.bat` ou ex√©cutez dans un terminal :
```cmd
start.bat
```

### macOS
```bash
chmod +x start.sh
./start.sh
```

### Ce qui se passe au premier d√©marrage

1. Le serveur Python (`server.py`) d√©marre sur le port **8765**
2. Le mod√®le Parakeet est t√©l√©charg√© depuis HuggingFace (~600 Mo, **une seule fois**)
3. La fen√™tre Electron s'ouvre apr√®s ~5 secondes
4. Le point **Serveur ASR** passe au vert quand le mod√®le est charg√© (~30s‚Äì2min selon votre machine)
5. Le point **LMStudio** passe au vert si le serveur LMStudio est actif sur le port 1234

### Premier enregistrement

1. Cliquez **‚ñ∂ D√©marrer** ‚Üí une modal s'ouvre
2. Saisissez le nom de l'entreprise et le titre de la r√©union
3. Cliquez **‚ñ∂ D√©marrer** dans la modal
4. Parlez ‚Äî la transcription appara√Æt en temps r√©el
5. Cliquez **‚ñ† Arr√™ter** pour terminer et g√©n√©rer le r√©sum√©

---

## 9. Lancement macOS (Apple M3)

Le script `start.sh` est l'√©quivalent de `start.bat` pour macOS.

```bash
./start.sh
```

**Ce que fait `start.sh` :**
- D√©tecte l'environnement virtuel Python (`venv/bin/python`) ou utilise `python3`
- Lance `server.py` en arri√®re-plan
- Attend 5 secondes
- Lance l'application Electron via `npm start` dans le dossier `electron/`

**Diff√©rences avec Windows :**
- Sur M3, la transcription s'effectue en **CPU** (ONNX Runtime ARM64) ‚Äî l√©g√®rement plus lent qu'avec un GPU NVIDIA, mais parfaitement utilisable
- Aucune installation de CUDA n√©cessaire
- Le mod√®le Parakeet TDT v3 tourne nativement sur Apple Silicon

---

## 10. Sources audio support√©es

| Source | Description |
|--------|-------------|
| **Micro syst√®me** | Microphone par d√©faut |
| **Micro X** | Tout p√©riph√©rique d'entr√©e audio d√©tect√© |
| **Audio syst√®me** | Capture l'audio de Zoom, Teams, Meet, etc. (via loopback) |
| **YouTube / URL Web** | Ouvre une fen√™tre navigateur, capture l'audio syst√®me |
| **Fichier audio** | Transcription d'un fichier wav, mp3, mp4, ogg |

> **Audio syst√®me sur macOS** : n√©cessite un pilote loopback tiers comme [BlackHole](https://github.com/ExistentialAudio/BlackHole) (gratuit).
> Installez BlackHole, puis dans les Pr√©f√©rences Son macOS cr√©ez un **Aggregate Device** combinant votre sortie habituelle + BlackHole. S√©lectionnez BlackHole comme source dans Parakeet.

---

## 11. FAQ / Probl√®mes courants

### Le point "Serveur ASR" reste rouge
- V√©rifiez que le serveur Python est d√©marr√©
- V√©rifiez que le port 8765 est libre :
  - Windows : `netstat -ano | findstr 8765`
  - macOS : `lsof -i :8765`
- Consultez les logs dans le terminal

### Erreur lors de l'installation de `onnx-asr[gpu,hub]`
- Assurez-vous que CUDA 11.8+ est install√© (Windows avec GPU NVIDIA)
- Sur macOS M3 ou PC sans GPU, utilisez `onnx-asr[hub]` (CPU uniquement)

### Le point "LMStudio" reste rouge
- V√©rifiez que LMStudio est ouvert et que le serveur local est d√©marr√© (onglet `<->`)
- V√©rifiez l'URL dans Parakeet : `http://localhost:1234/v1`
- Assurez-vous qu'un mod√®le est **charg√© et actif** dans LMStudio

### `npm install` √©choue dans `electron/`
- V√©rifiez la version Node.js : `node -v` doit √™tre ‚â• 18
- Supprimez `node_modules/` et relancez : `rm -rf node_modules && npm install`

### Aucun son captur√© depuis Zoom/Teams (macOS)
- Installez [BlackHole 2ch](https://github.com/ExistentialAudio/BlackHole) (gratuit)
- Dans les r√©glages son macOS, cr√©ez un **Aggregate Device** combinant votre micro + BlackHole
- S√©lectionnez cet Aggregate Device comme sortie dans Zoom/Teams
- Dans Parakeet, s√©lectionnez "BlackHole 2ch" comme source audio

### La transcription est lente
- Sur CPU (sans GPU), comptez ~2‚Äì5s de latence par chunk de 5s de parole
- Avec GPU NVIDIA, la latence tombe √† <1s
- Sur M3, la vitesse est comparable √† un CPU Intel Core i7 r√©cent (suffisant pour usage en r√©union)
